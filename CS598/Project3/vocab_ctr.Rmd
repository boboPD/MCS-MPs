---
title: "Vocabulary Construction"
author: "Pradyumna Das (pd10)"
date: "26/11/2020"
output: html_document
---
```{r load-packages, include=FALSE}
library(dplyr)
library(magrittr)
library(knitr)
library(tidytext)
```
## Vocabulary Construction

_The tidytext and dplyr packages were used in this exercise_

For vocabulary construction I just used the top words that had the most discrimination power towards positive/negative sentiment. I used the entire data to find the best 1000 such words. 500 words from the positive reviews and 500 from the negative ones. Below are the steps I followed:

First I read in the complete data, clean up the HTML and tokenise the reviews. I am considering unigrams and bigrams here. I had also tried with trigrams but there was barely any improvement in the models due to their addition and so I decided to use unigrams & bigrams only.

_I am using the same stopword list mentioned in the piazza post (with a few additions) since most out of box stopwords contain words that are important for sentiment analysis like don't, good, etc._
```{r}
mystopwords = c("i", "me", "my", "myself", 
               "we", "our", "ours", "ourselves", 
               "you", "your", "yours", 
               "their", "they", "his", "her", 
               "she", "he", "a", "an", "and",
               "is", "was", "are", "were", 
               "him", "himself", "has", "have", 
               "it", "its", "the", "us", "of", "in", "as", "by")
comp_data = read.table("alldata.tsv", header = T, stringsAsFactors = F)
comp_data$review = gsub('<.*?>', ' ', comp_data$review)
wf = comp_data %>% select(sentiment, review) %>% unnest_ngrams(word, review, n=2, n_min=1,ngram_delim="_", stopwords=mystopwords)
```

Then I counted how many times each term appeared in positive and negative reviews. Since some of the tokens have very few occurrences we remove them.
```{r}
cpos = wf %>% filter(sentiment==1) %>% count(word) %>% filter(n>100)
cneg = wf %>% filter(sentiment==0) %>% count(word) %>% filter(n>100)
```

Then to compare the counts of the words I joined the two sets and took the difference between the positive count and the negative count for each term. This difference is going to tell us which term exists primarily in the positive review and which ones in the negative reviews.

I had also tried to use the probability instead of raw counts. However that gave too much weightage to certain esoteric words that occurred only in one of the classes. The terms were positive in sentiment but it was unlikely that other users would use those words in reviews and that lead to a low test score. It made more sense to use words that were both positive and also common enough that I would find them in movie reviews.
```{r}
combined = full_join(cpos, cneg, by="word", suffix=c("_pos", "_neg"))
combined[is.na(combined[,"n_pos"]), "n_pos"] = 0
combined[is.na(combined[,"n_neg"]), "n_neg"] = 0
combined["diff"] = combined$n_pos - combined$n_neg
head(combined, 15)
```

Now all we need to do is sort the set by the difference column and pick the words that have the most positive diff and the ones that have the most negative diff.
```{r}
combined = arrange(combined, desc(diff))
```

The vocabulary file was created simply by appending the top and bottom 500 words of this matrix.
```{r eval=FALSE}
v=rbind(head(combined,500), tail(combined, 500))
readr::write_lines(v$word, "myvocab.txt")
```

Below are some sample terms from the positive and negative set:
```{r include=F}
pos = combined[1:500, "word"]
neg = tail(combined, 500)[, "word"]
positive_words = data.frame(unigrams=pos[!grepl("_", pos)][1:10], bigrams=pos[grepl("_", pos)][1:10])
negative_words = data.frame(unigrams=neg[!grepl("_", neg)][1:10], bigrams=neg[grepl("_", neg)][1:10])
```

Positive terms:
```{r echo=F}
positive_words
```
  
Negative terms:
```{r echo=F}
negative_words
```