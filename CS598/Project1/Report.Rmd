---
title: "Project 1 - Report"
output:
  html_document:
    df_print: paged
  pdf_document: default
---
###  Data preprocessing

#### Removing redundant columns
After going through the dataset I found a lot of the columns to be redundant. I have listed them below with the reasons for their removal

| Variables | Reason for removal |
| ---- | ---- |
| Garage_Area | This variable is correlated with garage cars |
| Garage_Cond | This variable is highly correlated with garage quality |
| BsmtFin_SF_1, Bsmt_Unf_SF, Bsmt_Half_Bath, BsmtFin_SF_2 | small correlation with sale price |
| Overall_Cond | Highly correlated with Overall_Qual |
| Year_Built | Added an age variable instead |
| Latitude, Longitude | Neighbourhood is better indicator |
| Pool_Qc, Pool_Area | Very few houses had pools so it wasn't a strong indicator |

The following variables were removed because they had the same value for nearly all of the rows:

Condition 2, Bsmt_Cond, Pool_Area, Land_Slope, Bldg_type, Utilities, Roof_Matl, Heating, Street, Misc_Feature, Misc_Val

#### Data Cleaning
Some of the columns seemed to have erroneous values and had to be fixed:

1. In some cases the Year_Built was greater than the Year_Sold which makes no sense so I just replaced those instances with Year_Sold
1. Garage_Yr_Blt had NAs mostly when the house did not have a Garage and so I replaced them with 0
1. All ordinal columns in the data were converted to integers to indicate order with Excellent = 5 going down to Poor = 1
1. Winsorised some of the columns according to the advice on Piazza and used the quantile values from the training set to winsorise the test set 
1. Create dummy variables for all nominal columns. Used the levels from the training data to create the dummies for the test set

### Tuning parameters

##### XGBoost
In order to tune the parameters of the boosted tree I used the train method in the caret package that can evaluate multiple parameters in one shot. I used 10 fold cross validation on one set of training data to find the best values for the eta, subsample, max_depth. Below is a table of the search space for each parameter and the result:

| Parameter | Search space | selected value |
| --- | --- | --- |
| eta (learning rate) | 0.1, 0.05, 0.01 | 0.05 |
| max_depth (tree depth) | 2, 3, 4, 5, 6 | 4 |
| subsample (fraction of rows randomly chosen to train tree) | 0.5, 0.75, 1 | 0.75 |

My model contains 2000 trees. I had tried a few values going from 5000 down to 1000 decreasing by 1000 at each step. I thought 2000 gave a good compromise between accuracy and running time. Beyond 2000 the difference in the actual predictions was around 1 dollar which is insignificant considering the price of a house.

##### Linear model
I used cv.glmnet to train a lasso model and used the min value of the lambda as my final choice since it gave the lowest RMSE across all the 10 splits. I also tried a few random values of the alpha (0.2, 0.5, 0.8) to train an elastic net model but lasso was always performing the best for me.

I was unable to bring the RMSE for the linear model under the benchmark even by using procedures exactly as described in the sample code. I am not sure why. That is why I decided to stick with my version.

### Learnings

I guess the most important thing that I learnt in this exercise is the importance of feature engineering. Really looking at the features to understand what aspects of the dependent variable they encode. I removed a lot more variables than the sample code shared on piazza with no significant loss in accuracy. I did not have the time to go through every variable and I believe had I done so I could have eliminated even more. I really think only about 15-20 of the nearly 80 predictors are actually useful in prediction. 

### Results

I am running this code on a Windows machine with R v4.0.2, 16GB RAM and an Intel i7 2.8Ghz dual core CPU. Below are my test results on the 10 train/test splits. The time shown below was for the entire training + prediction cycle. It does not include the time taken to evaluate the errors. It is measured in secs

```{r echo=FALSE}
results
```

