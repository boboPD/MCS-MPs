---
title: "Assignment 5"
author: "Pradyumna Das (pd10)"
date: "23/04/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Some libraries I used:
```{r}
library(ggplot2)
library(dplyr, warn.conflicts = F)
library(rjags)
```


Reading in the data
```{r}
parkvisitdata = read.csv("usparkvisits.csv")
```
Preparing the data for plotting
```{r}
data_tidy=gather(parkvisitdata, Year, Count, -Name)
data_tidy = mutate(data_tidy, log_Count=log(Count), Year=as.integer(substr(Year, 5, 8)))

plot_data = ggplot(data_tidy, mapping = aes(group=Name, color=Name))
```

Plotting the number of visits vs year
```{r}
plot_data + geom_line(mapping = aes(as.factor(Year), Count), show.legend = F) + xlab("Year")
```

Plotting the number of log(visits) vs year
```{r}
plot_data + geom_line(mapping = aes(as.factor(Year), log_Count), show.legend = F) + xlab("Year") + ylab("log(Count)")
```
Fitting a linear model per park:
```{r}
data_tidy$year_cent = data_tidy$Year - mean(data_tidy$Year)  #Centering the year column
coeffs = matrix(nrow = 30, ncol = 2, dimnames = list(parkvisitdata$Name, c("beta1", "beta2")))
for(park_idx in 1:30){
  coeffs[park_idx, ] = coef(lm(log_Count ~ year_cent, data_tidy[data_tidy$Name == parkvisitdata$Name[park_idx], c("log_Count", "year_cent")]))
}
```

Scatterplot of the coefficients:
```{r}
ggplot(as.data.frame(coeffs)) + geom_point(mapping = aes(beta1, beta2)) 
```
Average of the coefficients:
```{r}
apply(coeffs, 2, mean)
```
variance of the coefficients:
```{r}
apply(coeffs, 2, var)
```
correlation between the coefficients
```{r}
cor(coeffs[, 1], coeffs[, 2])
```

JAGS model code:
```
model{
  for(j in 1:30){
    for(i in 1:14){
      log_Count[j, i] ~ dnorm(beta[1, j] + beta[2,j]*year_cent[i], sigmasqyinv)
    }
    beta[1:2, j] ~ dmnorm(mubeta, Sigmabetainv)
  }
  
  mubeta = dmnorm(mubeta0, Sigmamubetainv)
  Sigmabetainv ~ dwish(2*Sigma0, 2)
  sigmasqyinv ~ dgamma(0.0001, 0.0001)
  Sigmabeta <- inverse(Sigmabetainv)
  rho <- Sigmabeta[1,2] / sqrt(Sigmabeta[1,1] * Sigmabeta[2,2])
  sigmasqy <- 1/sigmasqyinv
}
```

Setting up the data and initial values
```{r}
jags_data = list(
  log_Count=log(as.matrix(parkvisitdata[, -1])),
  year_cent = 2006:2019 - mean(2006:2019),
  mubeta0 = c(0,0),
  Sigmamubetainv = rbind(c(0.000001, 0),c(0, 0.000001)),
  Sigma0 = rbind(c(100, 0),c(0, 0.1))
)

inits1 <- list(
  list(sigmasqyinv = 10, mubeta = c(1000, 1000),
       Sigmabetainv = rbind(c(100, 0),c(0, 100))),
  list(sigmasqyinv = 0.001, mubeta = c(-1000, 1000),
       Sigmabetainv = rbind(c(100, 0),c(0, 100))),
  list(sigmasqyinv = 10, mubeta = c(1000, -1000),
       Sigmabetainv = rbind(c(0.001, 0),c(0, 0.001))),
  list(sigmasqyinv = 0.001, mubeta = c(-1000, -1000),
       Sigmabetainv = rbind(c(0.001, 0),c(0, 0.001))))

```

Running the JAGS model:
```{r}
jags_model = jags.model("parkmodel.bug", jags_data, inits = inits1, n.chains = 4)

update(jags_model, 3000) #burnin
```
Checking for convergence
```{r}
gelman.diag(coda.samples(jags_model, c("Sigmabeta", "rho", "sigmasqy"), 2000), autoburnin = F, multivariate = F)
```
Looks like the chains have not converged yet. Adding more burnin
```{r}
update(jags_model, 6000)
gelman.diag(coda.samples(jags_model, c("Sigmabeta", "rho", "sigmasqy"), 2000), autoburnin = F, multivariate = F)
```

The chains have converged now. Taking some samples for inference:
```{r}
samples = coda.samples(jags_model, c("Sigmabeta", "rho", "sigmasqy", "mubeta"), 10000)
effectiveSize(samples)
```

The effective sample sizes are well in excess of 4000. We can use these samples for inference

Summary: 
```{r}
summary(samples)
```

__95% central posterior interval for rho is about [-0.45, 0.23]__

density plot for rho
```{r}
densplot(samples[, "rho"])
```
Getting the samples as a matrix and calculating probability that rho < 0:
```{r}
samples_mat = as.matrix(samples)
mean(samples_mat[, "rho"] < 0)
```
There is approximately a 75% chance that rho < 0

95% posterior interval for $$e^{13\mu_{\beta_2}}$$:
```{r}
quantile(exp(13*samples_mat[, "mubeta[2]"]), c(0.025, 0.975))
```

DIC:
```{r}
dic.samples(jags_model, n.iter = 100000)
```
So the effective number of parameters is around 61.

JAGS model code for the second model:
```
model{
  for(j in 1:30){
    for(i in 1:14){
      log_Count[j, i] ~ dnorm(beta1[j] + beta2[j]*year_cent[i], sigmasqyinv)
    }
    beta1[j] ~ dnorm(mubeta1, 1/sigmasqbeta1)
    beta2[j] ~ dnorm(mubeta2, 1/sigmasqbeta2)
  }
  
  sigmasqyinv ~ dgamma(0.0001, 0.0001)
  sigmasqy <- 1/sigmasqyinv
  
  mubeta1 ~ dnorm(0, 0.000001)
  mubeta2 ~ dnorm(0, 0.000001)
  
  sigmabeta1 ~ dunif(0, 1000000)
  sigmabeta2 ~ dunif(0, 1000)
  sigmasqbeta1 <- sigmabeta1^2
  sigmasqbeta2 <- sigmabeta2^2
  
}
```

Setting up the data and initial values:
```{r}
jags_data2 = list(
  log_Count=log(as.matrix(parkvisitdata[, -1])),
  year_cent = 2006:2019 - mean(2006:2019)
)

#Decided the initial values based on the posteriors of the previous analysis
inits2 = list(
  list(mubeta1=1000, mubeta2=1000, sigmabeta1=50, sigmabeta2=50, sigmasqyinv=10),
  list(mubeta1=-1000, mubeta2=1000, sigmabeta1=0.0001, sigmabeta2=50, sigmasqyinv=0.001),
  list(mubeta1=1000, mubeta2=-1000, sigmabeta1=50, sigmabeta2=0.0001, sigmasqyinv=10),
  list(mubeta1=-1000, mubeta2=-1000, sigmabeta1=0.0001, sigmabeta2=0.0001, sigmasqyinv=0.001)
)
```

Running the JAGS model:
```{r}
jags_model2 = jags.model("parkmodel2.bug", jags_data2, inits = inits2, n.chains = 4)

update(jags_model2, 3000) #burnin
```

checking for convergence
```{r}
gelman.diag(coda.samples(jags_model2, c("mubeta1", "mubeta2", "sigmasqbeta1", "sigmasqbeta2", "sigmasqy"), n.iter = 2000), autoburnin = F)
```

Looks like the chains are still far from converging. Based on trial and error I found that this model required a really large number of iterations to converge.
```{r}
update(jags_model2, 15000000)
gelman.diag(coda.samples(jags_model2, c("mubeta1", "mubeta2", "sigmasqbeta1", "sigmasqbeta2", "sigmasqy"), n.iter = 2000), autoburnin = F)
```
Now the chains seem to have somewhat converged. Taking 10000 samples for inference:
```{r}
samples2 = coda.samples(jags_model2, c("mubeta1", "mubeta2", "sigmasqbeta1", "sigmasqbeta2", "sigmasqy"), n.iter = 10000)
effectiveSize(samples2)
```

the effective sizes are all greater than 4000 so ew should be ok. Summary of the samples:
```{r}
summary(samples2)
```

The values for $$\mu_{\beta_1}, \mu_{\beta_2} \space and\space  \sigma^2_y$$ largely agree with the previous model.

95% posterior interval for $$e^{13\mu_{\beta_2}}$$:
```{r}
samples_mat2 = as.matrix(samples2)
quantile(exp(13*samples_mat2[, "mubeta2"]), c(0.025, 0.975))
```
Comparing the two densities:
```{r}
vals = rbind(data.frame(values=exp(13*samples_mat[, "mubeta[2]"]), model="model1"), data.frame(values=exp(13*samples_mat2[, "mubeta2"]), model="model2"))
ggplot(vals) + geom_density(mapping = aes(values, colour=model))
```

The density for the second model is narrower and far more peaked. Maybe our prior was too informative?

DIC: 
```{r}
dic.samples(jags_model2, n.iter = 100000)
```
The effective number of parameters in this model is slightly lower, approx 58 compared to 61 in the previous model. This is to be expected since the covariances are not present in this model. The deviance is also lower for this model but only marginally. 43 for model 1 vs 41 for model 2. So we can say that both models are nearly equivalent (model 2 is very slightly better). 